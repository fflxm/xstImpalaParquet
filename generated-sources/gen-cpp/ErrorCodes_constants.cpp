/**
 * Autogenerated by Thrift Compiler (0.11.0)
 *
 * DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING
 *  @generated
 */
#include "ErrorCodes_constants.h"

namespace impala {

const ErrorCodesConstants g_ErrorCodes_constants;

ErrorCodesConstants::ErrorCodesConstants() {
  TErrorMessage.push_back("");
  TErrorMessage.push_back("<UNUSED>");
  TErrorMessage.push_back("$0");
  TErrorMessage.push_back("Cancelled");
  TErrorMessage.push_back("$0");
  TErrorMessage.push_back("$0");
  TErrorMessage.push_back("$0");
  TErrorMessage.push_back("$0");
  TErrorMessage.push_back("$0");
  TErrorMessage.push_back("$0");
  TErrorMessage.push_back("Parquet files should not be split into multiple hdfs-blocks. file=$0");
  TErrorMessage.push_back("Column metadata states there are $0 values, but read $1 values from column $2. file=$3");
  TErrorMessage.push_back("(unused)");
  TErrorMessage.push_back("ParquetScanner: reached EOF while deserializing data page header. file=$0");
  TErrorMessage.push_back("Metadata states that in group $0($1) there are $2 rows, but $3 rows were read.");
  TErrorMessage.push_back("(unused)");
  TErrorMessage.push_back("File '$0' column '$1' does not have the decimal precision set.");
  TErrorMessage.push_back("File '$0' column '$1' has a precision that does not match the table metadata precision. File metadata precision: $2, table metadata precision: $3.");
  TErrorMessage.push_back("File '$0' column '$1' does not have converted type set to DECIMAL");
  TErrorMessage.push_back("File '$0' column '$1' contains decimal data but the table metadata has type $2");
  TErrorMessage.push_back("Problem parsing file $0 at $1$2");
  TErrorMessage.push_back("Decompressor: block size is too big.  Data is likely corrupt. Size: $0");
  TErrorMessage.push_back("Decompressor: invalid compressed length.  Data is likely corrupt.");
  TErrorMessage.push_back("Snappy: GetUncompressedLength failed");
  TErrorMessage.push_back("SnappyBlock: RawUncompress failed");
  TErrorMessage.push_back("Snappy: Decompressed size is not correct.");
  TErrorMessage.push_back("Reserved resource size ($0) is larger than query mem limit ($1), and will be restricted to $1. Configure the reservation size by setting RM_INITIAL_MEM.");
  TErrorMessage.push_back("Cannot perform join at hash join node with id $0. The input data was partitioned the maximum number of $1 times. This could mean there is significant skew in the data or the memory limit is set too low.");
  TErrorMessage.push_back("Cannot perform aggregation at hash aggregation node with id $0. The input data was partitioned the maximum number of $1 times. This could mean there is significant skew in the data or the memory limit is set too low.");
  TErrorMessage.push_back("Builtin '$0' with symbol '$1' does not exist. Verify that all your impalads are the same version.");
  TErrorMessage.push_back("RPC Error: $0");
  TErrorMessage.push_back("RPC recv timed out: dest address: $0, rpc: $1");
  TErrorMessage.push_back("Failed to verify function $0 from LLVM module $1, see log for more details.");
  TErrorMessage.push_back("File $0 corrupt. RLE level data bytes = $1");
  TErrorMessage.push_back("Column '$0' has conflicting Avro decimal types. Table schema $1: $2, file schema $1: $3");
  TErrorMessage.push_back("Column '$0' has conflicting Avro decimal types. Declared $1: $2, $1 in table's Avro schema: $3");
  TErrorMessage.push_back("Unresolvable types for column '$0': table type: $1, file type: $2");
  TErrorMessage.push_back("Unresolvable types for column '$0': declared column type: $1, table's Avro schema type: $2");
  TErrorMessage.push_back("Field $0 is missing from file and default values of type $1 are not yet supported.");
  TErrorMessage.push_back("Inconsistent table metadata. Mismatch between column definition and Avro schema: cannot read field $0 because there are only $1 fields.");
  TErrorMessage.push_back("Field $0 is missing from file and does not have a default value.");
  TErrorMessage.push_back("Field $0 is nullable in the file schema but not the table schema.");
  TErrorMessage.push_back("Inconsistent table metadata. Field $0 is not a record in the Avro schema.");
  TErrorMessage.push_back("Could not read definition level, even though metadata states there are $0 values remaining in data page. file=$1");
  TErrorMessage.push_back("Mismatched number of values in column index $0 ($1 vs. $2). file=$3");
  TErrorMessage.push_back("File '$0' is corrupt: error decoding dictionary-encoded value of type $1 at offset $2");
  TErrorMessage.push_back("SSL private-key password command ('$0') failed with error: $1");
  TErrorMessage.push_back("The SSL certificate path is blank");
  TErrorMessage.push_back("The SSL private key path is blank");
  TErrorMessage.push_back("The SSL certificate file does not exist at path $0");
  TErrorMessage.push_back("The SSL private key file does not exist at path $0");
  TErrorMessage.push_back("SSL socket creation failed: $0");
  TErrorMessage.push_back("Memory allocation of $0 bytes failed");
  TErrorMessage.push_back("Could not read repetition level, even though metadata states there are $0 values remaining in data page. file=$1");
  TErrorMessage.push_back("File '$0' has an incompatible Parquet schema for column '$1'. Column type: $2, Parquet schema:\n$3");
  TErrorMessage.push_back("Failed to allocate $0 bytes for collection '$1'.\nCurrent buffer size: $2 num tuples: $3.");
  TErrorMessage.push_back("Temporary device for directory $0 is blacklisted from a previous error and cannot be used.");
  TErrorMessage.push_back("Temporary file $0 is blacklisted from a previous error and cannot be expanded.");
  TErrorMessage.push_back("RPC client failed to connect: $0");
  TErrorMessage.push_back("Metadata for file '$0' appears stale. Try running \"refresh $1\" to reload the file metadata.");
  TErrorMessage.push_back("File '$0' has an invalid Parquet version number: $1.\nPlease check that it is a valid Parquet file. This error can also occur due to stale metadata. If you believe this is a valid Parquet file, try running \"refresh $2\".");
  TErrorMessage.push_back("Tried to read $0 bytes but could only read $1 bytes. This may indicate data file corruption. (file $2, byte offset: $3)");
  TErrorMessage.push_back("Invalid read of $0 bytes. This may indicate data file corruption. (file $1, byte offset: $2)");
  TErrorMessage.push_back("File '$0' has an invalid version header: $1\nMake sure the file is an Avro data file.");
  TErrorMessage.push_back("$0's allocations exceeded memory limits.");
  TErrorMessage.push_back("No longer in use.");
  TErrorMessage.push_back("For better performance, snappy-, gzip-, and bzip-compressed files should not be split into multiple HDFS blocks. file=$0 offset $1");
  TErrorMessage.push_back("$0 Data error, likely data corrupted in this block.");
  TErrorMessage.push_back("$0 Decompressor error at $1, code=$2");
  TErrorMessage.push_back("Decompression failed to make progress, but end of input is not reached. File appears corrupted. file=$0");
  TErrorMessage.push_back("Unexpected end of compressed file. File may be truncated. file=$0");
  TErrorMessage.push_back("Sender$0 timed out waiting for receiver fragment instance: $1, dest node: $2");
  TErrorMessage.push_back("Kudu type $0 is not available in Impala.");
  TErrorMessage.push_back("Impala type $0 is not available in Kudu.");
  TErrorMessage.push_back("Not in use.");
  TErrorMessage.push_back("Kudu features are disabled by the startup flag --disable_kudu.");
  TErrorMessage.push_back("Cannot perform hash join at node with id $0. Repartitioning did not reduce the size of a spilled partition. Repartitioning level $1. Number of rows $2:\n$3\n$4");
  TErrorMessage.push_back("Not in use.");
  TErrorMessage.push_back("File '$0' is corrupt: truncated data block at offset $1");
  TErrorMessage.push_back("File '$0' is corrupt: invalid union value $1 at offset $2");
  TErrorMessage.push_back("File '$0' is corrupt: invalid boolean value $1 at offset $2");
  TErrorMessage.push_back("File '$0' is corrupt: invalid length $1 at offset $2");
  TErrorMessage.push_back("File '$0' is corrupt: invalid encoded integer at offset $1");
  TErrorMessage.push_back("File '$0' is corrupt: invalid record count $1 at offset $2");
  TErrorMessage.push_back("File '$0' is corrupt: invalid compressed block size $1 at offset $2");
  TErrorMessage.push_back("File '$0' is corrupt: invalid metadata count $1 at offset $2");
  TErrorMessage.push_back("File '$0' could not be read: string $1 was longer than supported limit of $2 bytes at offset $3");
  TErrorMessage.push_back("File '$0' is corrupt: error decoding value of type $1 at offset $2");
  TErrorMessage.push_back("File '$0' is corrupt: error reading dictionary for data of type $1: $2");
  TErrorMessage.push_back("Length of column is $0 which exceeds maximum supported length of 2147483647 bytes.");
  TErrorMessage.push_back("Scratch space limit of $0 bytes exceeded for query while spilling data to disk on backend $1.");
  TErrorMessage.push_back("Unexpected error allocating $0 byte buffer: $1");
  TErrorMessage.push_back("File '$0' is corrupt: metadata indicates a zero row count but there is at least one non-empty row group.");
  TErrorMessage.push_back("Cannot schedule query: no registered backends available.");
  TErrorMessage.push_back("Key already present in Kudu table '$0'.");
  TErrorMessage.push_back("Not found in Kudu table '$0': $1");
  TErrorMessage.push_back("Error in Kudu table '$0': $1");
  TErrorMessage.push_back("Column '$0': unsupported Avro type '$1'");
  TErrorMessage.push_back("Column '$0': invalid Avro decimal type with precision = '$1' scale = '$2'");
  TErrorMessage.push_back("Row with null value violates nullability constraint on table '$0'.");
  TErrorMessage.push_back("Parquet file '$0' column '$1' contains an out of range timestamp. The valid date range is 1400-01-01..9999-12-31.");
  TErrorMessage.push_back("Could not create files in any configured scratch directories (--scratch_dirs=$0) on backend '$1'. $2 of scratch is currently in use by this Impala Daemon ($3 by this query). See logs for previous errors that may have prevented creating or writing scratch files. The following directories were at capacity: $4");
  TErrorMessage.push_back("Error reading $0 bytes from scratch file '$1' on backend $2 at offset $3: could only read $4 bytes");
  TErrorMessage.push_back("Kudu table '$0' column '$1' contains an out of range timestamp. The valid date range is 1400-01-01..9999-12-31.");
  TErrorMessage.push_back("Row of size $0 could not be materialized by $1. Increase the max_row_size query option (currently $2) to process larger rows.");
  TErrorMessage.push_back("Failed to verify generated IR function $0, see log for more details.");
  TErrorMessage.push_back("Failed to get minimum memory reservation of $0 on daemon $1:$2 for query $3 due to following error: $4Memory is likely oversubscribed. Reducing query concurrency or configuring admission control may help avoid this error.");
  TErrorMessage.push_back("Rejected query from pool $0: $1");
  TErrorMessage.push_back("Admission for query exceeded timeout $0ms in pool $1. Queued reason: $2 Additional Details: $3");
  TErrorMessage.push_back("Failed to create thread $0 in category $1: $2");
  TErrorMessage.push_back("Disk I/O error on $0: $1");
  TErrorMessage.push_back("DataStreamRecvr for fragment=$0, node=$1 is closed already");
  TErrorMessage.push_back("Kerberos principal should be of the form: <service>/<hostname>@<realm> - got: $0");
  TErrorMessage.push_back("The input size is too large for LZ4 compression: $0");
  TErrorMessage.push_back("InitAuth() called multiple times with different names. Was called with $0. Now using $1.");
  TErrorMessage.push_back("Can not read Parquet file $0 with deprecated BIT_PACKED encoding for rep or def levels. Support was removed in Impala 3.0 - see IMPALA-6077.");
  TErrorMessage.push_back("Row batch cannot be serialized: size of $0 bytes exceeds supported limit of $1");
  TErrorMessage.push_back("The library $0 last modified time $1 does not match the expected last modified time $2. Run 'refresh functions <db name>'.");
  TErrorMessage.push_back("Error reading $0 bytes from scratch file '$1' on backend $2 at offset $3: verification of read data failed.");
  TErrorMessage.push_back("Cancelled in $0");
  TErrorMessage.push_back("Server is being shut down: $0.");
  TErrorMessage.push_back("Parquet file '$0' column '$1' contains a timestamp with invalid time of day. The time of day should be 0 <= and < 24 hour (in nanoseconds).");
  TErrorMessage.push_back("File '$0' is corrupt: error decoding BOOLEAN value with encoding $1 at offset $2");
  TErrorMessage.push_back("Failed to submit $0 to thread pool after waiting $1 seconds");
  TErrorMessage.push_back("$0 failed to finish before the $1 second timeout");
  TErrorMessage.push_back("Failed due to unreachable impalad(s): $0");
  TErrorMessage.push_back("Session expired due to inactivity");
  TErrorMessage.push_back("Query $0 expired due to client inactivity (timeout is $1)");
  TErrorMessage.push_back("Query $0 expired due to execution time limit of $1");
  TErrorMessage.push_back("Query $0 terminated due to CPU limit of $1");
  TErrorMessage.push_back("Query $0 terminated due to scan bytes limit of $1");
  TErrorMessage.push_back("Query $0 terminated due to rows produced limit of $1. Unset or increase NUM_ROWS_PRODUCED_LIMIT query option to produce more rows.");
  TErrorMessage.push_back("Expression rewrite rejected due to result size ($0) exceeding the limit ($1).");
  TErrorMessage.push_back("Query $0 cancelled due to unresponsive backend: $1 has not sent a report in $2ms (max allowed lag is $3ms)");
  TErrorMessage.push_back("Parquet file '$0' column '$1' contains an out of range date. The valid date range is 0001-01-01..9999-12-31.");
  TErrorMessage.push_back("Session closed because it has no active connections");
  TErrorMessage.push_back("The user authorized on the connection '$0' does not match the session username '$1'");
  TErrorMessage.push_back("$0 failed with error: $1");
  TErrorMessage.push_back("LZ4Block: Decompressed size is not correct.");
  TErrorMessage.push_back("LZ4Block: Invalid input length.");
  TErrorMessage.push_back("LZ4Block: Invalid compressed length.  Data is likely corrupt.");
  TErrorMessage.push_back("LZ4: LZ4_decompress_safe failed");
  TErrorMessage.push_back("LZ4: LZ4_compress_default failed");
  TErrorMessage.push_back("Statement length of $0 bytes exceeds the maximum statement length ($1 bytes)");
  TErrorMessage.push_back("Avro file '$0' is corrupt: out of range date value $1 at offset $2. The valid date range is -719162..2932896 (0001-01-01..9999-12-31).");
  TErrorMessage.push_back("ORC file '$0' column '$1' contains an out of range timestamp. The valid date range is 1400-01-01..9999-12-31.");
  TErrorMessage.push_back("ORC file '$0' column '$1' contains an out of range date. The valid date range is 0001-01-01..9999-12-31.");
  TErrorMessage.push_back("File '$0' has an incompatible ORC schema for column '$1', Column type: $2, ORC schema: $3");
  TErrorMessage.push_back("Root of the $0 type returned by the ORC lib is not STRUCT: $1. Either there are bugs in the ORC lib or ORC file '$2' is corrupt.");
  TErrorMessage.push_back("Unable to perform Null-Aware Anti-Join. Could not get enough reservation to fit all rows with NULLs from the build side in memory. Memory required for $0 rows was $1. $2/$3 of the join's reservation was available for the rows.");
  TErrorMessage.push_back("Invalid or unknown query handle: $0.");
  TErrorMessage.push_back("Query $0 terminated due to join rows produced exceeds the limit of $1 at node with id $2. Unset or increase JOIN_ROWS_PRODUCED_LIMIT query option to produce more rows.");
  TErrorMessage.push_back("Query execution failure caused by local disk IO fatal error on backend: $0.");
  TErrorMessage.push_back("Error parsing JWKS: $0.");
  TErrorMessage.push_back("Error verifying JWT Token: $0.");
  TErrorMessage.push_back("Couldn't skip rows in column '$0' in file '$1'.");

}

} // namespace

